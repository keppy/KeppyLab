{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"describe\u2022","text":"<p>Infrastructure as conversation. Build complete systems in minutes, not hours.</p> Get Started \u2192 View on GitHub <p>Watch me build a complete RAG system in 7 prompts:</p> <pre><code>1. \"Find me filesystem and database servers\"\n2. \"Install them\"\n3. \"Create an embedding schema\"  \n4. \"Read my documents\"\n5. \"Index them\"\n6. \"Make it searchable\"\n7. \"Give me a terminal interface\"\n</code></pre> <p>     \u23f1\ufe0f Time elapsed: 6 minutes, 23 seconds   </p> <p>     \u2705 Result: Production RAG system. Running. Queryable. Complete.   </p> <p>\"Wait... that's impossible.\"</p> <p>But here's the thing that breaks your brain:</p> <p>This isn't a demo. This isn't a mockup. This isn't theoretical.</p> <p>Right now, as you read this, nerds are having conversations that spawn infrastructure. Databases appear from sentences. APIs emerge from descriptions. Complete systems from pure conversation.</p>"},{"location":"#the-reality-check","title":"\ud83e\udd2f The Reality Check","text":"<p>You just witnessed the death of configuration files. Infrastructure is now a conversation.</p>"},{"location":"#the-lab-behind-the-magic","title":"The Lab Behind the Magic","text":"<p>KeppyLab builds the impossible. We're the research lab where infrastructure becomes conversation, where you can build anything by describing it.</p> <p>More from the lab:</p> <p>WorldEnder.ai \ud83c\udf0e RAG-powered text adventures - Because why shouldn't extinction scenarios have perfect narrative coherence?</p> <p>Disease Lab \ud83e\uddea Knowledge graph AI for rare diseases - Turning medical literature into discovery engines</p> <p>cobol-reporter \ud83d\udd2d A RAG and report generator for understanding COBOL code - Making legacy systems comprehensible through AI</p> <p>Join the builders: Monthly Seattle AI Hackers &amp; Paper Readers</p>"},{"location":"#stay-in-the-loop","title":"Stay in the Loop     This page was built by conversation.","text":"<p>Weekly breakthroughs \u2192 Papers, advances, and \"wait, that's possible now?\" moments delivered to your inbox: </p> Join KeppyLab AI Mailing ListPlease wait...<p>Thanks! We'll be in touch!</p><p>Oops! Something went wrong, please try again</p>  \u2190 Back  <p>     Oh, and one more thing...   </p> <p>     Everything you just read? Created through prompts.   </p> <p>       &gt; Human: \"update the homepage for describe\"       &gt; Claude: Updating with new branding and messaging...       &gt; Human: \"make it feel impossible but real\"       &gt; Claude: Adding cognitive dissonance...       &gt; Human: \"ship it\"       &gt; Claude: \u2705 Reality updated </p> <p>     The conversation you just witnessed? That's how everything gets built now. </p> <p>Hope to see you around, fly dangerous </p> <p>@keppylab_ai | Github | LinkedIn | Hugging Face</p>"},{"location":"contact/","title":"Contact","text":""},{"location":"contact/#connect-with-me","title":"Connect with Me","text":"<p>Drop me a line james@keppylab.com or find me on these platforms:</p> <p>@keppylab_ai | Github | LinkedIn | Hugging Face</p>"},{"location":"describe/","title":"describe\u2022","text":"<p>Watch me build a complete RAG system in 7 prompts:</p> <pre><code>1. \"Find me filesystem and database servers\"\n2. \"Install them\"\n3. \"Create an embedding schema\"\n4. \"Read my documents\"\n5. \"Index them\"\n6. \"Make it searchable\"\n7. \"Give me a terminal interface\"\n</code></pre> <p>Result: Working RAG system. No config files. No documentation. Just conversation.</p>"},{"location":"describe/#install","title":"Install","text":"<pre><code>npm install -g @keppylab/describe\n</code></pre> <p>Add to Claude Desktop: <pre><code>{\n  \"describe\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@keppylab/describe\"]\n  }\n}\n</code></pre></p> <p>Restart Claude. Start describing.</p>"},{"location":"describe/#what-this-is","title":"What This Is","text":"<p>Infrastructure used to require configuration. Now it requires conversation.</p> <p>The difference isn't speed. It's that you never leave the conversation.</p>"},{"location":"describe/#examples","title":"Examples","text":""},{"location":"describe/#github-pr-analyzer","title":"GitHub PR Analyzer","text":"<pre><code>\"Install github and code-analysis servers\"\n\"Connect to my repo\"\n\"Analyze open PRs for complexity\"\n\"Flag potential issues\"\n\"Show me the results\"\n</code></pre>"},{"location":"describe/#sql-chat-interface","title":"SQL Chat Interface","text":"<pre><code>\"Install sqlite server\"\n\"Connect to my database\"\n\"Let me query it in plain English\"\n\"Show results as tables\"\n</code></pre>"},{"location":"describe/#csv-to-api","title":"CSV to API","text":"<pre><code>\"Install filesystem and data servers\"\n\"Read all CSVs from ~/data\"\n\"Create REST endpoints for each\"\n\"Add search functionality\"\n\"Start the server\"\n</code></pre>"},{"location":"describe/#the-shift","title":"The Shift","text":"<p>Before: Learn syntax \u2192 Write config \u2192 Debug \u2192 Deploy</p> <p>After: Describe what you want \u2192 It exists \u2192 Iterate by talking</p> <p>This changes who can build: - Researchers can build knowledge bases - Designers can create data pipelines - Anyone can build what they imagine</p> <p>The barrier isn't syntax anymore. It's imagination.</p>"},{"location":"describe/#how-it-works","title":"How It Works","text":"<p>When you say \"install filesystem server\", describe: 1. Searches the MCP registry 2. Installs the server 3. Configures it automatically 4. Makes it available to Claude</p> <p>You don't need to know this. You just need to know what you want.</p>"},{"location":"describe/#commands","title":"Commands","text":"You Say What Happens \"Search for database servers\" Shows available options \"Install server-name\" Installs and configures \"Show installed servers\" Lists what you have \"Remove server-name\" Uninstalls cleanly"},{"location":"describe/#self-modifying-infrastructure","title":"Self-Modifying Infrastructure","text":"<pre><code>Human: My RAG system is too slow\nClaude: I see it's scanning all documents on each query. Adding caching...\nClaude: Try now - should be 10x faster\n</code></pre> <p>The system sees its own state and improves itself through conversation.</p>"},{"location":"describe/#technical-details","title":"Technical Details","text":"<ul> <li>Python async/await core</li> <li>Official MCP registry integration</li> <li>npm, Docker, git installation support</li> <li>Cross-platform config discovery</li> <li>Isolated virtual environment</li> <li>Full backup/restore</li> </ul> <p>Requirements: Python 3.9+, Node.js 18+, Claude Desktop</p>"},{"location":"describe/#philosophy","title":"Philosophy","text":"<p>We're not building better tools. We're removing tools entirely.</p> <p>Infrastructure should be a conversation, not a configuration.</p> <p>GitHub \u2022 Discord \u2022 @keppylab_ai</p> <p>The best interface is no interface. The best configuration is conversation.</p>"},{"location":"blog/","title":"Writings &amp; Research","text":"<p>Findings and discoveries documented in written form for your consumption </p>"},{"location":"blog/#writing","title":"Writing","text":"<ul> <li>Research to the People &amp; Stanford Medicine's Rare Disease AI Hackathon</li> <li>I Demoed Too Early</li> <li>Fine Tuning Pythia by Hand and Calculating PPL</li> </ul>"},{"location":"blog/#notebooks","title":"Notebooks","text":"<ul> <li>AGI</li> <li>WorldEnder.ai</li> <li>Byte-Pair Encoding Tokenization for Ancient Greek</li> </ul>"},{"location":"blog/#models-runs","title":"Models &amp; Runs","text":"<ul> <li>pythia-70m-dedupe-yt</li> </ul>"},{"location":"blog/2023/06/24/fine-tuning-pythia-by-hand-and-calculating-ppl/","title":"Fine Tuning Pythia by Hand and Calculating PPL","text":"<p>6/24/2023</p> <p>This is an introduction on how to get your feet wet writing a training loop to fine tune a GPT-NeoX model, specifically the 70m deduped Pythia model. After fine tuning, I measure and compare the PPL to gpt2 and do some rough plots to contrast the two models. The notebook can run on a free colab GPU instance so this is great for just a proof of concept that we can in fact fine tune the model on new data.</p> <p>Disclaimer: this model is not intended for production use</p> Resource type URI Model https://huggingface.co/keppy/pythia-70m-dedupe-yt Notebook https://gist.github.com/keppy/a5be88ea59a67a901571b6e0c3478585"},{"location":"blog/2023/06/24/fine-tuning-pythia-by-hand-and-calculating-ppl/#fine-tuning","title":"Fine Tuning","text":"<p>Some of the more interesting bits from the fine tuning and metric calculations follow below.</p> <p>This bit of code called <code>group_texts()</code> can be found at https://huggingface.co/docs/transformers/tasks/language_modeling: <pre><code>block_size = 128\n\ndef group_texts(examples):\n    # Concatenate all texts.\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n    # customize this part to your needs.\n    if total_length &gt;= block_size:\n        total_length = (total_length // block_size) * block_size\n    # Split by chunks of block_size.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n</code></pre> We have to run our tokenized text examples through this function to turn them into blocks of text that we can train on. So you can't just tokenize the data and then use that in your training loop. This is obvious to me now but when I was new to the API I found it odd that you cart this function around copy paste style.</p> <p>If we take a look at the actual training loop we see that we take our batches from the <code>train_dataloader</code> which was built using <code>group_texts()</code>: <pre><code>model.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n</code></pre></p>"},{"location":"blog/2023/06/24/fine-tuning-pythia-by-hand-and-calculating-ppl/#evaluation","title":"Evaluation","text":"<p>By now we have done our transfer learning (I'm not that old I swear) and we have our model as the result. So how do we evaluate this causal model? We want to get the model's perplexity (PPL). I'll let you research more about what that means but basically there is a library from Hugging Face called evaluate which will let us compute PPL against models on the Hugging Face hub.</p> <p>What we do in the notebook is upload our newly trained model to the hub. Then, after doing a round of batching and evaluation, we can compute two separate metrics--one for gpt2 and one for our model to compare. We do this by adding batches of decoded tokens (predictions) to our metric evaluators inside the eval loop and then computing the metrics after the loop has finished.</p> <p><pre><code># batching, eval, using metric.add_batch()......\n# ...\ngpt2_PPL = gpt2_metric.compute(model_id=\"gpt2\")\nyt_PPL = yt_metric.compute(model_id=\"keppy/pythia-70m-dedupe-yt\")\n</code></pre> Notice how we just have to pass model IDs to compute. This allows us to compare the PPL of these two models and we can graph it.</p> <p></p> <p>The metric gives us also a mean perplexity so we can show that as some bars as well.</p> <p></p> <p>Feel free to copy the notebook from the gist to your own colab. Hopefully this is enough info to get you started. Going through the notebook will take you step by step, and it doesn't take long to run as I've used a very small subset of the dataset.</p>"},{"location":"blog/2024/06/10/i-demoed-too-early/","title":"I Demoed Too Early","text":"<p>Last Friday I demoed my game WorldEnder.ai and it wasn't ready yet. I had too many slides, which were useless in the demo setting, and I was rushing to explain the core invention. If I could do it over again I wouldn\u2019t change a thing.</p> <p>After the demo, everyone I talked to was engaged in my problem space and had ideas that saved me from potential design decisions that would eat up time. Because I showed a notebook and quickly talked through several lines of code, the feedback I got and the ideas we brainstormed didn\u2019t feel threatening to the project or like a waste of time. It is still early enough to make any change we want without much cost. The algorithm and gameplay driving mechanism, which unanimously got positive reactions, drove discussion about exciting possibilities and even got a few people wanting to contribute.</p> <p>Over the weekend I got a contributor and a PR, and we are going to push it further at a hackathon next weekend. If you have a chance to demo, find something to demo and do it. You might find your team that way.</p>"},{"location":"blog/2024/07/03/research-to-the-people-and-stanford-medicines-rare-disease-ai-hackathon/","title":"Research to the People and Stanford Medicine's Rare Disease AI Hackathon","text":"<p>A week ago I flew down to SF to present at Github HQ for a rare disease AI hackathon aiming to jump-start research in applying LLMs and ML in general to the task of rare disease diagnosis and treatment. The two diseases being researched were Ehlers-Danlos and Hypophosphatasia(HPP). Over two months, my team focused solely on HPP. We developed a fine tuned model for answering questions about HPP using the ~1300 paper data source that was provided by the event organizers (shout-out to Pete Kane).</p> <p>Initially I undertook the task of implementing techniques from KG RAG and then building upon them to integrate a knowledge graph that QIAGEN provided for the hackathon. Unfortunately this was more than I could do in the given time as the QIAGEN knowledge graph was not a drop in replacement for the SPOKE knowledge graph. I think there is still a lot of milage to be gained here though. If disease-lab can harness SPOKE and provide a portable Python library for querying it that integrates with vector storage, the interface could be extended to other knowledge graphs as well. The still messy part seems to be in writing the kg building, querying, and pruning code.</p> <p>We came up with a fine tuned sentence embedding model. The papers were in PDF format so we had to invent some data cleaning and chunking methods. Ultimately we ended up with a fine tuned mxbai-embed-large-v1 model and were able to embed the entire dataset. On top of this we built a RAG system which is able to cite the specific sources in the corpus of embedded texts and provide a provenance section in each generation. We utilized Meta-Llama-3-8B-Instruct as our foundational model.</p> <p>We also produced a fined tuned version of the Meta-Llama-3-8B-Instruct transformer model that is in need of further evaluation. To decipher if it is truly better than rag with the baseline foundational model, we need to test the model with/without RAG and with/without fine tuning.</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/rag/","title":"RAG","text":""},{"location":"blog/category/rare-diseases/","title":"rare diseases","text":""},{"location":"blog/category/worldenderai/","title":"WorldEnder.ai","text":""},{"location":"blog/category/demos/","title":"demos","text":""},{"location":"blog/category/blogets/","title":"blogets","text":""},{"location":"blog/category/fine-tuning/","title":"fine tuning","text":""},{"location":"blog/category/pythia/","title":"Pythia","text":""},{"location":"blog/category/ppl/","title":"PPL","text":""}]}